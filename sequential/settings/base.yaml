# Base settings for common model & experiment

experiment_name: "DOCA gen standard"

# Model settings
model_name: "DOCA4Rec"

model_dataset:
<<<<<<< HEAD
  # GenDataset : generative image, DescriptionDataset : image description 
  train_dataset: "GenDataset"     
  # TestGenDataset : generative image, TestDescriptionDataset : image description 
  test_dataset: "TestGenDataset"
=======
  # GenDataset : generative image, DescriptionDataset : image description
  train_dataset: "DescriptionDataset"
  # TestGenDataset : generative image, TestDescriptionDataset : image description
  test_dataset: "TestDescriptionDataset"
>>>>>>> 065bcf9bebab5a612283c6dccaae4ca47505751e

model_arguments:
  hidden_size: 512        # hidden size for multi head attention
  num_attention_heads: 4  # number of heads for multi head attention
  num_hidden_layers: 2    # number of transformer layers
  max_len: 90             # length of input sequence
  dropout_prob: 0.2       # dropout probability
  hidden_act: "gelu"      # Activation function for attention, mlp
  pos_emb: True           # True if using positional embedding
<<<<<<< HEAD
  mask_prob: 0.5          # Input sequence masking probability
=======
  mask_prob: 0.4          # Input sequence masking probability
  model_ckpt_path: "./model/0427000421/model_val_9.835206767016421.pt"
>>>>>>> 065bcf9bebab5a612283c6dccaae4ca47505751e

  num_mlp_layers: 3       # number of mlp layers
  merge: "concat"            # ways to merge output of bert4rec and image embeddings. One of ["concat", "mul"].

  detail_text: False      # using embedding of detail_text of item
  std:  0.075               # noise std(available if std value is minus, the std is adjusting std )
  mean: 0.0               # noise mean
<<<<<<< HEAD
  
  gen_img: True           # group by same image description
=======

  gen_img: False           # group by same image description
  closest_origin: False    # the gen image embedding closest to origin image embedding
>>>>>>> 065bcf9bebab5a612283c6dccaae4ca47505751e

  neg_sampling: False


lr: 1.0e-4
lr_step: 25

multi_optim: False
second_lr: 1.0e-4
lr_milestones : [25,50,75]
lr_encoder_gamma: 0.1


epoch: 100
batch_size: 64
weight_decay: 0.001 # weight decay for Adam
num_workers: 4
valid_step: 3 # number of steps to run validation

data_local: False       # True if loading dataset from local directory, else download from huggingface hub
data_repo: "sequential"
dataset: "small"
data_version: "a884caecb21336efb478cc7cd4e34a6ec2af2413"

n_cuda: "0"
